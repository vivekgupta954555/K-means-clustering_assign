{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566f4512-f0a6-42a9-958a-baf46bba9706",
   "metadata": {},
   "source": [
    "# QUES NO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd738d0-3226-40db-9d83-a5d2e3e9daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are basically three types of the clustering  algorithm\n",
    "1-KMeans clustering \n",
    "2-Hirrarchal clustering\n",
    "3-DBSCAN clustering \n",
    "\n",
    " KMeans clustering\n",
    "  The algorithm works as follows:  \n",
    "\n",
    "1-First, we randomly initialize k points, called means or cluster centroids.\n",
    "2-We categorize each item to its closest mean and we update the mean’s coordinates,\n",
    "which are the averages of the items categorized in that cluster so far.\n",
    "3-We repeat the process for a given number of iterations and at the end, we have our\n",
    "clusters.\n",
    "\n",
    " Hirrarchal clustering\n",
    "  Hierarchical clustering is a method of cluster analysis in data mining that creates\n",
    "a hierarchical representation of the clusters in a dataset. The method starts by\n",
    "treating each data point as a separate cluster and then iteratively combines the\n",
    "closest clusters until a stopping criterion is reached. The result of hierarchical\n",
    "clustering is a tree-like structure, called a dendrogram, which illustrates the\n",
    "hierarchical relationships among the clusters\n",
    "\n",
    " DBSCAN\n",
    "   Density-based spatial clustering of applications with noise (DBSCAN) clustering method\n",
    " Clusters are dense regions in the data space, separated by regions of the lower density\n",
    "    of points. The DBSCAN algorithm is based on this intuitive notion of “clusters” and \n",
    "    “noise”. The key idea is that for each point of a cluster, the neighborhood of \n",
    "    a given radius has to contain at least a minimum number of points. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8205f88-c2f7-4f75-a8ed-e8835e329e8a",
   "metadata": {},
   "source": [
    "# QUES NO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb694fd1-cfe5-460e-8231-229c29521401",
   "metadata": {},
   "outputs": [],
   "source": [
    "K-means is a centroid-based clustering algorithm, where we calculate the distance between\n",
    "each data point and a centroid to assign it to a cluster. The goal is to identify the \n",
    "K number of groups in the dataset. \n",
    "\n",
    "How does K-means work\n",
    "\n",
    " The algorithm can be step down into 4-5 steps. \n",
    "\n",
    "1-Choosing the number of clusters \n",
    "The first step is to define the K number of clusters in which we will group the data.\n",
    "\n",
    "2-Initializing centroids\n",
    "Centroid is the center of a cluster but initially, the exact center of data points will\n",
    "be unknown so, we select random data points and define them as centroids for each\n",
    "cluster. We will initialize 3 centroids in the dataset.\n",
    "\n",
    "3-Assign data points to the nearest cluster\n",
    "Now that centroids are initialized, the next step is to assign data points Xn to their\n",
    "closest cluster centroid Ck\n",
    "In this step, we will first calculate the distance between data point X and centroid\n",
    "C using Euclidean Distance metric.\n",
    "And then choose the cluster for data points where the distance between the data point\n",
    "and the centroid is minimum.\n",
    "\n",
    "4-Re-initialize centroids \n",
    "Next, we will re-initialize the centroids by calculating the average of all data points\n",
    "of that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f33756-1f5e-4c22-91e5-794b9049eaae",
   "metadata": {},
   "source": [
    "# QUES NO 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa8ff7-5c18-4379-88de-c7d3e5edf1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantage from the KMeans\n",
    "\n",
    "Relatively simple to implement.\n",
    "\n",
    "Scales to large data sets.\n",
    "\n",
    "Guarantees convergence.\n",
    "\n",
    "Can warm-start the positions of centroids.\n",
    "\n",
    "Easily adapts to new examples\n",
    "\n",
    "Limitation\n",
    "\n",
    "Limitation 1: K-means requires us to specify the number of clusters a priory.\n",
    "\n",
    "Limitation 2: K-Means is sensitive towards outlier. Outliers can skew the clusters in\n",
    "K-Means in very large extent.\n",
    "\n",
    "Limitation 3: K-Means forms spherical clusters only. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b35a43-e3da-4386-b1b8-ec7a8b2bec1c",
   "metadata": {},
   "source": [
    "# QUES NO 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d29e5-a7f1-4595-a53e-da05505ac6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elbow method\n",
    "to define clusters such that the total intra-cluster variation [or total within-cluster\n",
    "sum of square (WSS)] is minimized. The total wss measures the compactness of the \n",
    "clustering, and we want it to be as small as possible. The elbow method runs k-means \n",
    "clustering (kmeans number of clusters) on the dataset for a range of values of k (say 1\n",
    " to 10) In the elbow method, we plot mean distance and look for the elbow point where \n",
    "the rate of decrease shifts. For each k, calculate the total within-cluster sum of \n",
    "squares (WSS). This elbow point can be used to determine K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c14b62-fcf2-49a8-830f-7c7c67128d26",
   "metadata": {},
   "source": [
    "# QUES NO 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fa4df-d7a0-4567-829f-798f07f20145",
   "metadata": {},
   "outputs": [],
   "source": [
    "Applications of K-Means Clustering:\n",
    "k-means can be applied to data that has a smaller number of dimensions, is numeric,\n",
    "and is continuous. such as document clustering, identifying crime-prone areas, customer\n",
    "segmentation, insurance fraud detection, public transport data analysis, clustering of\n",
    "IT alerts…etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8471ccf-ba1e-40ba-91e9-b0cfbce59b05",
   "metadata": {},
   "source": [
    "# QUES NO 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1329e1e9-cded-4971-a6c5-6a98df49150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster analysis is a great technique for identifying groups with similar patterns. \n",
    "However, once clusters are formed, it can remain challenging to determine the driving\n",
    "features behind the clusters. But this step is crucial to reveal valuable insights\n",
    "that may have been missed before and can be used for decision-making and a deeper\n",
    "understanding of your data set. One manner to determine the driving features is by\n",
    "coloring the samples on the feature values. Although this is insightful, it is \n",
    "labor-intensive when there are hundreds of features. In addition, the exact contribution\n",
    "of a (set of) feature(s) can be difficult to judge with clusters of different sizes \n",
    "and densities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442f109-475a-4e44-9e82-5bbcdd50c422",
   "metadata": {},
   "source": [
    "# QUES NO 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e39c6c-746f-48b7-ac4c-470eee664bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Following are the challenges faced by K-Means Clustering:\n",
    "\n",
    "1-k-Means doesn’t perform well if the clusters have varying sizes, different densities,\n",
    "or non-spherical shapes.\n",
    "2-Has to be run for a certain amount of iteration or it would produce a suboptimal result.\n",
    "3-Computationally expensive as distance is to be calculated from each centroid to all \n",
    "data points.\n",
    "4-Can’t handle high dimensional data\n",
    "5-Number of clusters need to be specified\n",
    "6-Since initial centroids are arbitrarily chosen, the result is not exactly replicable.\n",
    "Shows out of memory error for large datase\n",
    "\n",
    "How to overcome by all the above disadvantage\n",
    "\n",
    "1: Try to implement the advanced techniques such as Gaussian Mixture Models or\n",
    "Hierarchical Clustering, which can adapt to more complex data distributions \n",
    "and hierarchies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992a491-87b4-4f15-8032-14660bc489b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6619b7-1bd2-4143-be98-ac6077c7f68d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5f37a-f0ce-45aa-b5cc-6c318fb1af21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f825a-99e7-412b-9847-86a0f08a3375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
